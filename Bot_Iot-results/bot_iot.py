# -*- coding: utf-8 -*-
"""bot-iot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DYZSutxliKND1wvlCZYmVJzxmjLMAnvf
"""

# Mount Google Drive for Dataset Access
#from google.colab import files
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, GaussianNoise, Lambda, Layer
from tensorflow.keras.models import Model
from tensorflow.keras import backend as K
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical
from cleverhans.tf2.attacks.fast_gradient_method import fast_gradient_method
from cleverhans.tf2.attacks.projected_gradient_descent import projected_gradient_descent
from collections import defaultdict, Counter
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.lines import Line2D
from sklearn.manifold import TSNE
import logging
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import BinaryCrossentropy
from tqdm import tqdm

# Logging configuration
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
tf.random.set_seed(RANDOM_STATE)

# 1. Data Preparation Functions
def load_and_prepare_data(filepath="Bot-IoT-Training.csv"):
    """Rectified pipeline: load, clean, explore, balance, feature engineering, split."""
    # Load dataset
    dataset = pd.read_csv(filepath)
    print(f"\nRaw dataset shape: {dataset.shape}")

    # Clean: drop missing and duplicate rows
    dataset.dropna(inplace=True)
    dataset.drop_duplicates(inplace=True)
    print(f"After cleaning: {dataset.shape}\n")

    # Explore dataset
    print("First five rows:")
    print(dataset.head())
    print("\nDataset info:")
    print(dataset.info())
    print("\nNull value count:")
    print(dataset.isna().sum())

    # Feature selection: Bot-IoT specific (drop unused columns by index)
    cols_to_drop = [0, 1, 2, 3, 4, 5, 17, 18]  # pkSeqID, proto, saddr, sport, daddr, dport, category, subcategory
    dataset1 = dataset.copy()
    dataset1.drop(dataset1.columns[cols_to_drop], axis=1, inplace=True)
    print("Shape after column drop: ", dataset1.shape)
    print(dataset1.iloc[0])

    # Separate features and label
    x = dataset1.drop('attack', axis='columns').copy()  # 10 features
    y = dataset1['attack'].copy()

    # Encode categorical features (if any remain after drop)
    encoders = {}
    for col in x.select_dtypes(include=['object']).columns:
        encoder = LabelEncoder()
        x[col] = encoder.fit_transform(x[col])
        encoders[col] = encoder

    # Normalize features (MinMax to [0,1], as in original)
    scaler = MinMaxScaler()
    x_scaled = scaler.fit_transform(x)

    # Handle imbalance: Apply SMOTE to oversample minority (normal) class
    print("\nOriginal class split:", Counter(y))
    smote = SMOTE(random_state=42)
    x_balanced, y_balanced = smote.fit_resample(x_scaled, y)
    print("Balanced class split:", Counter(y_balanced))

    # Label encoding for the output (after balancing)
    label_encoder = LabelEncoder()
    labels_encoded = label_encoder.fit_transform(y_balanced)
    labels_categorical = to_categorical(labels_encoded)
    print(f"\nFeature matrix shape: {x_balanced.shape}")
    print(f"Labels shape: {labels_categorical.shape}")
    print(f"Class mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}")

    # Optional: Undersample if dataset too large post-SMOTE (e.g., cap at 1M samples)
    if len(x_balanced) > 1000000:
        x_balanced, _, y_balanced, _ = train_test_split(
            x_balanced, labels_categorical,
            train_size=1000000/len(x_balanced),
            random_state=42,
            stratify=labels_categorical
        )
        labels_categorical = y_balanced  # Update

    # Stratified train-test split (further split into train, validation, test)
    X_temp, X_test, y_temp, y_test = train_test_split(
        x_balanced, labels_categorical, test_size=0.3,
        random_state=42, stratify=labels_categorical
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2,
        random_state=42, stratify=y_temp
    )

    print(f"\nTrain: {X_train.shape}, {y_train.shape}")
    print(f"Validation: {X_val.shape}, {y_val.shape}")
    print(f"Test: {X_test.shape}, {y_test.shape}")

    return X_train, X_val, X_test, y_train, y_val, y_test, scaler, label_encoder

# 2. GAN Implementation
class GAN:
    def __init__(self, input_dim, latent_dim=32):
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.generator = self.build_generator()
        self.discriminator = self.build_discriminator()
        self.gan = self.build_gan()

    def build_generator(self):
        model = tf.keras.Sequential([
            Dense(64, activation='relu', input_shape=(self.latent_dim,)),
            BatchNormalization(),
            Dense(128, activation='relu'),
            BatchNormalization(),
            Dense(self.input_dim, activation='tanh')
        ])
        return model

    def build_discriminator(self):
        model = tf.keras.Sequential([
            Dense(128, activation='relu', input_shape=(self.input_dim,)),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dropout(0.3),
            Dense(1, activation='sigmoid')
        ])
        model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002))
        return model

    def build_gan(self):
        self.discriminator.trainable = False
        model = tf.keras.Sequential([
            self.generator,
            self.discriminator
        ])
        model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002))
        return model

    def train(self, X_train, epochs=100, batch_size=128):
        y_real = np.ones((batch_size, 1))
        y_fake = np.zeros((batch_size, 1))

        for epoch in range(epochs):
            # Train discriminator
            idx = np.random.randint(0, X_train.shape[0], batch_size)
            real_samples = X_train[idx]

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            fake_samples = self.generator.predict(noise, verbose=0)

            d_loss_real = self.discriminator.train_on_batch(real_samples, y_real)
            d_loss_fake = self.discriminator.train_on_batch(fake_samples, y_fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # Train generator
            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            g_loss = self.gan.train_on_batch(noise, y_real)

            if epoch % 10 == 0:
                logger.info(f"GAN Epoch {epoch}/{epochs} [D loss: {d_loss:.4f}] [G loss: {g_loss:.4f}]")

    def generate(self, X_clean, epsilon=0.2):
        noise = np.random.normal(0, 1, (X_clean.shape[0], self.latent_dim))
        perturbations = self.generator.predict(noise, verbose=0)
        # Scale perturbations to epsilon range
        perturbations = epsilon * (perturbations - perturbations.min()) / (perturbations.max() - perturbations.min())
        return np.clip(X_clean + perturbations, X_clean.min(), X_clean.max())

# 3. Model and Attack/Defense Framework
class VAESampling(Layer):
    def call(self, inputs):
        z_mean, z_log_var = inputs
        batch = tf.shape(z_mean)[0]
        dim = tf.shape(z_mean)[1]
        epsilon = K.random_normal(shape=(batch, dim))
        return z_mean + tf.exp(0.5 * z_log_var) * epsilon

class VAEModel(Model):
    def __init__(self, encoder, decoder, beta=0.5, **kwargs):
        super(VAEModel, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.beta = beta
        self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
        self.kl_loss_tracker = tf.keras.metrics.Mean(name="kl_loss")

    @property
    def metrics(self):
        return [
            self.total_loss_tracker,
            self.reconstruction_loss_tracker,
            self.kl_loss_tracker,
        ]

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstruction = self.decoder(z)
        return reconstruction

    def train_step(self, data):
        if isinstance(data, tuple): data = data[0]
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(data)
            reconstruction = self.decoder(z)
            reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(
                    tf.square(data - reconstruction), axis=1
                )
            )
            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(
                    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1
                )
            )
            total_loss = reconstruction_loss + self.beta * kl_loss
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        return {
            "loss": self.total_loss_tracker.result(),
            "reconstruction_loss": self.reconstruction_loss_tracker.result(),
            "kl_loss": self.kl_loss_tracker.result(),
        }

    def compile(self, optimizer, **kwargs):
        super().compile(optimizer=optimizer, loss=lambda y_true, y_pred: 0.0, **kwargs)

class JointModel(Model):
    def __init__(self, encoder, decoder, classifier, beta=0.5, **kwargs):
        super(JointModel, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder
        self.classifier = classifier
        self.beta = beta
        self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name="reconstruction_loss")
        self.classification_loss_tracker = tf.keras.metrics.Mean(name="classification_loss")
        self.kl_loss_tracker = tf.keras.metrics.Mean(name="kl_loss")
        self.accuracy_tracker = tf.keras.metrics.CategoricalAccuracy(name="accuracy")

    @property
    def metrics(self):
        return [
            self.total_loss_tracker,
            self.reconstruction_loss_tracker,
            self.classification_loss_tracker,
            self.kl_loss_tracker,
            self.accuracy_tracker,
        ]

    def call(self, inputs):
        z_mean, z_log_var, z = self.encoder(inputs)
        reconstruction = self.decoder(z)
        classification = self.classifier(z)
        return {'reconstruction': reconstruction, 'classification': classification}

    def train_step(self, data):
        if isinstance(data, tuple):
            x, y_dict = data
            y = y_dict.get('classification')
        else:
            x = data
            y = None
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = self.encoder(x)
            reconstruction = self.decoder(z)
            classification = self.classifier(z)
            reconstruction_loss = tf.reduce_mean(
                tf.reduce_sum(tf.square(x - reconstruction), axis=1))
            classification_loss = 0.0
            if y is not None:
                epsilon = tf.keras.backend.epsilon()
                classification = tf.clip_by_value(classification, epsilon, 1. - epsilon)
                classification_loss = tf.reduce_mean(
                    tf.reduce_sum(-y * tf.math.log(classification), axis=1))
            kl_loss = -0.5 * tf.reduce_mean(
                tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))
            total_loss = (0.3 * reconstruction_loss +
                        1.0 * classification_loss +
                        self.beta * kl_loss)
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        self.total_loss_tracker.update_state(total_loss)
        self.reconstruction_loss_tracker.update_state(reconstruction_loss)
        self.classification_loss_tracker.update_state(classification_loss)
        self.kl_loss_tracker.update_state(kl_loss)
        if y is not None:
            self.accuracy_tracker.update_state(y, classification)
        return {m.name: m.result() for m in self.metrics}

    def compile(self, optimizer, **kwargs):
        super().compile(
            optimizer=optimizer,
            loss={
                'reconstruction': lambda y_true, y_pred: 0.0,
                'classification': lambda y_true, y_pred: 0.0
            },
            metrics={
                'classification': ['accuracy']
            },
            **kwargs
        )

class ImprovedAdversarialDefenseFramework:
    def __init__(self, config=None):
        self.config = config or {
            'test_split': 0.3,
            'validation_split': 0.2,
            'batch_size': 32,
            'max_epochs': 12,
            'early_stopping_patience': 3,
            'regularization_strength': 0.1,
            'dropout_rate': 0.6,
            'noise_level': 0.3,
            'learning_rate': 0.0002,
            'vae_beta': 0.5,
            'dynamic_threshold': True,
            'joint_train_epochs': 30,
            'strong_attack_epsilons': [0.5, 0.7, 1.0],
            'epsilon_ranges': {
                'fgsm': [0.1, 0.2, 0.3, 0.4, 0.5],
                'pgd': [0.1, 0.2, 0.3, 0.4, 0.5],
                'blackbox': [0.05, 0.1, 0.2, 0.3],
                'gan': [0.2, 0.3, 0.4, 0.5],
                'noise': [0.4, 0.6, 0.8, 1.0]
            },
            'threshold_tuning_range': np.arange(50, 96, 5)  # Percentiles to try
        }
        self.results = defaultdict(lambda: defaultdict(dict))
        self.trained_models = {}
        self.feature_names = []
        self.tuned_thresholds = {}  # Store tuned thresholds per attack type
        self.gan = None  # Will be initialized later

    def build_regularized_classifier(self, input_shape, name="classifier"):
        """Build classifier with heavy regularization"""
        input_layer = Input(shape=input_shape, name=f'{name}_input')
        x = GaussianNoise(self.config['noise_level'])(input_layer)
        x = Dense(24, activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(self.config['regularization_strength']))(x)
        x = BatchNormalization()(x)
        x = Dropout(self.config['dropout_rate'])(x)
        x = Dense(12, activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(self.config['regularization_strength']))(x)
        x = BatchNormalization()(x)
        x = Dropout(self.config['dropout_rate'])(x)
        output = Dense(2, activation='softmax', name=f'{name}_output')(x)
        model = Model(inputs=input_layer, outputs=output, name=name)
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate']),
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        return model

    def build_autoencoder(self, input_size, name="autoencoder"):
        """Build Variational Autoencoder (VAE)"""
        # Encoder
        encoder_input = Input(shape=(input_size,), name=f'{name}_input')
        x = GaussianNoise(0.1)(encoder_input)
        x = Dense(32, activation='relu', name='encoder_1')(x)
        x = Dropout(0.3)(x)
        z_mean = Dense(8, name='z_mean')(x)
        z_log_var = Dense(8, name='z_log_var')(x)
        z = VAESampling()([z_mean, z_log_var])
        encoder = Model(encoder_input, [z_mean, z_log_var, z], name='encoder')

        # Decoder
        decoder_input = Input(shape=(8,), name='decoder_input')
        x = Dense(32, activation='relu', name='decoder_1')(decoder_input)
        x = Dropout(0.3)(x)
        decoder_output = Dense(input_size, activation='linear', name=f'{name}_output')(x)
        decoder = Model(decoder_input, decoder_output, name='decoder')

        # Full VAE model
        vae = VAEModel(encoder, decoder, beta=self.config['vae_beta'], name=name)
        vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))
        return vae, encoder, decoder

    def build_joint_model(self, input_size, name="joint_model"):
        """Build joint model with VAE components"""
        # Encoder
        encoder_input = Input(shape=(input_size,), name='reconstruction')
        x = GaussianNoise(0.1)(encoder_input)
        x = Dense(32, activation='relu',
                kernel_regularizer=tf.keras.regularizers.l2(0.05))(x)
        x = BatchNormalization()(x)
        x = Dropout(0.4)(x)
        z_mean = Dense(8, name='z_mean')(x)
        z_log_var = Dense(8, name='z_log_var')(x)
        z = VAESampling()([z_mean, z_log_var])
        encoder = Model(encoder_input, [z_mean, z_log_var, z], name='joint_encoder')

        # Reconstruction branch
        decoder_input = Input(shape=(8,), name='decoder_input')
        x = Dense(32, activation='relu')(decoder_input)
        x = Dropout(0.3)(x)
        decoder_output = Dense(input_size, activation='linear', name='reconstruction')(x)
        decoder = Model(decoder_input, decoder_output, name='joint_decoder')

        # Classification branch
        classifier_input = Input(shape=(8,), name='classifier_input')
        x = Dense(16, activation='relu',
                 kernel_regularizer=tf.keras.regularizers.l2(0.05))(classifier_input)
        x = Dropout(0.5)(x)
        classifier_output = Dense(2, activation='softmax', name='classification')(x)
        classifier = Model(classifier_input, classifier_output, name='joint_classifier')

        # Full joint model
        joint_model = JointModel(encoder, decoder, classifier, beta=self.config['vae_beta'], name=name)
        joint_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))
        return joint_model, encoder, decoder, classifier

    def train_with_overfitting_detection(self, model, X_train, y_train, name):
        """Train with strict overfitting detection"""
        logger.info(f"Training {name}...")
        class OverfittingCallback(tf.keras.callbacks.Callback):
            def __init__(self, patience=2, threshold=0.05):
                self.patience = patience
                self.threshold = threshold
                self.wait = 0
                self.best_val_acc = 0
            def on_epoch_end(self, epoch, logs=None):
                val_acc = logs.get('val_accuracy', 0)
                train_acc = logs.get('accuracy', 0)
                if val_acc > 0.97:
                    logger.warning(f"Stopping {self.model.name}: val_accuracy too high ({val_acc:.3f})")
                    self.model.stop_training = True
                    return
                if (train_acc - val_acc) > self.threshold:
                    self.wait += 1
                    if self.wait >= self.patience:
                        logger.warning(f"Stopping {self.model.name}: overfitting detected")
                        self.model.stop_training = True
                else:
                    self.wait = 0

        history = model.fit(
            X_train, y_train,
            batch_size=self.config['batch_size'],
            epochs=self.config['max_epochs'],
            validation_split=self.config['validation_split'],
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_accuracy', mode='max',
                    patience=self.config['early_stopping_patience'],
                    restore_best_weights=True
                ),
                OverfittingCallback(patience=2, threshold=0.1)
            ],
            verbose=1
        )
        return model

    def train_autoencoder(self, X_train, model_name="autoencoder"):
        """Train VAE on both clean and adversarial examples"""
        baseline = self.trained_models['baseline']
        X_adv_fgsm = self.generate_fgsm(baseline, X_train, 0.2)
        X_adv_pgd = self.generate_pgd(baseline, X_train, 0.2)
        X_adv_blackbox = self.generate_blackbox(baseline, X_train, 0.2)
        X_adv_gan = self.generate_gan(baseline, X_train, 0.2)
        X_adv_noise = self.generate_noise(baseline, X_train, 0.2)

        # Combine clean and adversarial data
        X_combined = np.concatenate([X_train, X_adv_fgsm, X_adv_pgd, X_adv_blackbox, X_adv_gan, X_adv_noise])
        y_combined = np.concatenate([X_train, X_train, X_train, X_train, X_train, X_train])  # Autoencoder target

        # Shuffle
        indices = np.random.permutation(len(X_combined))
        X_combined, y_combined = X_combined[indices], y_combined[indices]

        # Train VAE
        autoencoder, encoder, decoder = self.build_autoencoder(input_size=X_train.shape[1])
        autoencoder.fit(
            X_combined, y_combined,
            batch_size=self.config['batch_size'],
            epochs=25,
            validation_split=0.2,
            verbose=1,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_reconstruction_loss', mode='min',
                    patience=5, restore_best_weights=True
                )
            ]
        )
        self.trained_models[model_name] = autoencoder
        self.trained_models[f'{model_name}_encoder'] = encoder
        self.trained_models[f'{model_name}_decoder'] = decoder

    def tune_dynamic_thresholds(self, X_val, y_val):
        """Tune dynamic thresholds using validation set"""
        logger.info("Tuning dynamic thresholds...")
        autoencoder = self.trained_models['autoencoder']
        encoder = self.trained_models['autoencoder_encoder']

        # Generate adversarial examples for validation set
        baseline = self.trained_models['baseline']
        attacks = {
            'fgsm': self.generate_fgsm,
            'pgd': self.generate_pgd,
            'blackbox': self.generate_blackbox,
            'gan': self.generate_gan,
            'noise': self.generate_noise
        }

        # Store best thresholds for each attack type
        best_thresholds = {}

        for attack_name, attack_func in attacks.items():
            logger.info(f"Tuning threshold for {attack_name}...")
            best_f1 = 0
            best_percentile = 65  # Default

            # Generate adversarial examples
            X_adv = attack_func(baseline, X_val, 0.3)  # Use medium epsilon

            # Get reconstructions
            z_mean, z_log_var, z = encoder.predict(X_adv, verbose=0)
            X_recon = autoencoder.decoder.predict(z, verbose=0)
            recon_errors = np.mean(np.abs(X_adv - X_recon), axis=1)

            # Try different percentiles
            for percentile in self.config['threshold_tuning_range']:
                threshold = np.percentile(recon_errors, percentile)
                ood_mask = recon_errors > threshold

                # Apply defense
                X_defended = X_adv.copy()
                X_defended[ood_mask] = X_recon[ood_mask]

                # Evaluate
                y_pred = baseline.predict(X_defended, verbose=0)
                y_pred_classes = np.argmax(y_pred, axis=1)
                y_true_classes = np.argmax(y_val, axis=1)

                f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')

                if f1 > best_f1:
                    best_f1 = f1
                    best_percentile = percentile

            best_thresholds[attack_name] = best_percentile
            logger.info(f"Best threshold for {attack_name}: {best_percentile} (F1: {best_f1:.4f})")

        self.tuned_thresholds = best_thresholds
        logger.info("Threshold tuning completed")

    def train_all_models(self, X_train, y_train, X_val, y_val):
        """Complete training pipeline with proper symbolic tensor handling"""
        logger.info("Training all defense models...")

        # Convert inputs to float32 numpy arrays
        X_train = np.array(X_train, dtype=np.float32)
        y_train = np.array(y_train)
        X_val = np.array(X_val, dtype=np.float32)
        y_val = np.array(y_val)

        # Convert y_train to one-hot if needed
        if len(y_train.shape) == 1:
            y_train = to_categorical(y_train)
        y_train = y_train.astype(np.float32)

        if len(y_val.shape) == 1:
            y_val = to_categorical(y_val)
        y_val = y_val.astype(np.float32)

        # 1. Baseline model
        logger.info("Training baseline classifier...")
        baseline = self.build_regularized_classifier(
            input_shape=(X_train.shape[1],), name="baseline"
        )
        self.trained_models['baseline'] = self.train_with_overfitting_detection(
            baseline, X_train, y_train, "baseline"
        )

        # 2. Adversarial training models
        logger.info("Training adversarial training models...")
        for i, ratio in enumerate([0.1, 0.2, 0.3], 1):
            logger.info(f"Training adversarial model {i} with {ratio*100}% adversarial data...")
            clean_size = int(len(X_train) * (1 - ratio))
            X_clean, y_clean = X_train[:clean_size], y_train[:clean_size]
            X_adv_base, y_adv = X_train[clean_size:], y_train[clean_size:]

            X_adv_fgsm = self.generate_fgsm(self.trained_models['baseline'], X_adv_base, 0.2)
            X_adv_pgd = self.generate_pgd(self.trained_models['baseline'], X_adv_base, 0.2)
            X_adv_blackbox = self.generate_blackbox(self.trained_models['baseline'], X_adv_base, 0.2)
            X_adv_gan = self.generate_gan(self.trained_models['baseline'], X_adv_base, 0.2)
            X_adv_noise = self.generate_noise(self.trained_models['baseline'], X_adv_base, 0.2)

            mix_size = len(X_adv_base) // 2
            X_adv_mixed = np.concatenate([
                X_adv_fgsm[:mix_size], X_adv_pgd[:mix_size],
                X_adv_blackbox[:mix_size], X_adv_gan[:mix_size], X_adv_noise[:mix_size]
            ])
            y_adv_mixed = np.concatenate([
                y_adv[:mix_size], y_adv[:mix_size],
                y_adv[:mix_size], y_adv[:mix_size], y_adv[:mix_size]
            ])

            X_combined = np.concatenate([X_clean, X_adv_mixed])
            y_combined = np.concatenate([y_clean, y_adv_mixed])

            indices = np.random.permutation(len(X_combined))
            X_combined, y_combined = X_combined[indices], y_combined[indices]

            adv_model = self.build_regularized_classifier(
                input_shape=(X_train.shape[1],), name=f"adversarial_{i}"
            )
            self.trained_models[f'adversarial_training_{i}'] = self.train_with_overfitting_detection(
                adv_model, X_combined, y_combined, f"adversarial_training_{i}"
            )

        # 3. Improved autoencoder training
        self.train_autoencoder(X_train)

        # 4. Tune dynamic thresholds
        self.tune_dynamic_thresholds(X_val, y_val)

        # 5. Train GAN
        logger.info("Training GAN...")
        self.gan = GAN(input_dim=X_train.shape[1])
        self.gan.train(X_train, epochs=50, batch_size=128)

        # 6. Joint model with end-to-end training
        logger.info("Training improved joint model...")
        joint_model, joint_encoder, joint_decoder, joint_classifier = self.build_joint_model(input_size=X_train.shape[1])

        # Generate adversarial examples for joint training
        X_adv_fgsm = self.generate_fgsm(self.trained_models['baseline'], X_train, 0.2)
        X_adv_pgd = self.generate_pgd(self.trained_models['baseline'], X_train, 0.2)
        X_adv_blackbox = self.generate_blackbox(self.trained_models['baseline'], X_train, 0.2)
        X_adv_gan = self.generate_gan(self.trained_models['baseline'], X_train, 0.2)
        X_adv_noise = self.generate_noise(self.trained_models['baseline'], X_train, 0.2)

        # Create combined dataset
        X_combined = np.concatenate([X_train, X_adv_fgsm, X_adv_pgd, X_adv_blackbox, X_adv_gan, X_adv_noise])
        y_combined = np.concatenate([y_train, y_train, y_train, y_train, y_train, y_train])

        # Convert to tensors right before training
        X_combined = tf.convert_to_tensor(X_combined, dtype=tf.float32)
        y_combined = tf.convert_to_tensor(y_combined, dtype=tf.float32)

        # Train joint model with dictionary output
        joint_model.fit(
            X_combined,
            {'classification': y_combined, 'reconstruction': X_combined},
            batch_size=self.config['batch_size'],
            epochs=self.config['joint_train_epochs'],
            validation_split=0.2,
            verbose=1,
            callbacks=[
                tf.keras.callbacks.EarlyStopping(
                    monitor='val_accuracy',
                    mode='max',
                    patience=8,
                    restore_best_weights=True
                )
            ]
        )
        self.trained_models['joint_model'] = joint_model
        self.trained_models['joint_encoder'] = joint_encoder
        self.trained_models['joint_decoder'] = joint_decoder
        self.trained_models['joint_classifier'] = joint_classifier

        logger.info("✓ All models trained successfully")

    def generate_fgsm(self, model, X, epsilon):
        """Generate FGSM attack"""
        X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
        adv_X = fast_gradient_method(
            model, X_tensor, eps=epsilon, norm=np.inf,
            clip_min=X.min(), clip_max=X.max()
        )
        return adv_X.numpy()

    def generate_pgd(self, model, X, epsilon):
        """Generate PGD attack"""
        X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
        adv_X = projected_gradient_descent(
            model, X_tensor, eps=epsilon, eps_iter=epsilon/10,
            nb_iter=20, norm=np.inf,
            clip_min=X.min(), clip_max=X.max()
        )
        return adv_X.numpy()

    def generate_blackbox(self, model, X, epsilon):
        """Generate black box attack"""
        noise = np.random.uniform(-epsilon, epsilon, X.shape)
        return np.clip(X + noise, X.min(), X.max())

    def generate_gan(self, model, X, epsilon):
        """Generate GAN attack using trained GAN"""
        if self.gan is None:
            # Fallback to original method if GAN not trained
            structured = np.sin(X * 3) * epsilon * 0.3
            random_noise = np.random.normal(0, epsilon * 0.4, X.shape)
            return np.clip(X + structured + random_noise, X.min(), X.max())
        return self.gan.generate(X, epsilon)

    def generate_noise(self, model, X, epsilon):
        """Generate noise attack"""
        noise = np.random.normal(0, epsilon, X.shape)
        return np.clip(X + noise, X.min(), X.max())

    def apply_ood_defense(self, X_adv, attack_type=None):
        """Apply OOD defense with dynamic thresholding"""
        autoencoder = self.trained_models['autoencoder']
        encoder = self.trained_models['autoencoder_encoder']

        z_mean, z_log_var, z = encoder.predict(X_adv, verbose=0)
        X_recon = autoencoder.decoder.predict(z, verbose=0)

        # Calculate reconstruction error
        recon_errors = np.mean(np.abs(X_adv - X_recon), axis=1)

        # Use tuned thresholds if available, otherwise fall back to defaults
        if self.config['dynamic_threshold'] and attack_type and attack_type in self.tuned_thresholds:
            threshold_percentile = self.tuned_thresholds[attack_type]
        else:
            # Default thresholds
            thresholds = {
                'fgsm': 70, 'pgd': 75, 'blackbox': 75,
                'gan': 75, 'noise': 70
            }
            threshold_percentile = thresholds.get(attack_type, 65)

        threshold = np.percentile(recon_errors, threshold_percentile)
        ood_mask = recon_errors > threshold

        # Apply denoising
        X_defended = X_adv.copy()
        X_defended[ood_mask] = X_recon[ood_mask]
        detection_rate = np.mean(ood_mask)
        return X_defended, detection_rate

    def apply_combined_defense(self, X_adv, attack_type=None):
        """Apply combined defense with attack-aware processing"""
        try:
            # Step 1: OOD denoising with dynamic threshold
            X_denoised, detection_rate = self.apply_ood_defense(X_adv, attack_type)

            # Step 2: Use joint model for final prediction
            joint_model = self.trained_models['joint_model']
            z_mean, z_log_var, z = joint_model.encoder.predict(X_denoised, verbose=0)
            classification_preds = joint_model.classifier.predict(z, verbose=0)
            return classification_preds, detection_rate
        except Exception as e:
            logger.error(f"Combined defense error: {str(e)}")
            # Fallback to OOD defense only
            X_defended, detection_rate = self.apply_ood_defense(X_adv, attack_type)
            baseline_model = self.trained_models['baseline']
            preds = baseline_model.predict(X_defended, verbose=0)
            return preds, detection_rate

    def comprehensive_evaluation(self, X_test, y_test):
        """Complete evaluation with enhanced attack testing and metrics"""
        logger.info("Starting comprehensive evaluation...")
        attacks = {
            'fgsm': self.generate_fgsm,
            'pgd': self.generate_pgd,
            'blackbox': self.generate_blackbox,
            'gan': self.generate_gan,
            'noise': self.generate_noise
        }

        # Include stronger epsilon values
        for attack in ['fgsm', 'pgd']:
            self.config['epsilon_ranges'][attack].extend(self.config['strong_attack_epsilons'])

        defenses = ['baseline', 'adversarial_training_1', 'adversarial_training_2',
                   'adversarial_training_3', 'ood_denoising', 'combined_defense']
        results = []
        confusion_matrices = {}

        baseline_model = self.trained_models['baseline']
        y_true = np.argmax(y_test, axis=1)

        # Clean baseline evaluation
        clean_preds = np.argmax(baseline_model.predict(X_test, verbose=0), axis=1)
        clean_acc = accuracy_score(y_true, clean_preds)
        clean_f1 = f1_score(y_true, clean_preds, average='weighted')
        logger.info(f"Clean baseline accuracy: {clean_acc:.3f}, F1: {clean_f1:.3f}")

        for attack_name, attack_func in attacks.items():
            logger.info(f"Evaluating {attack_name.upper()}...")
            for epsilon in self.config['epsilon_ranges'][attack_name]:
                logger.info(f"  Testing epsilon = {epsilon}")
                # Generate adversarial samples
                X_adv = attack_func(baseline_model, X_test, epsilon)

                for defense in defenses:
                    try:
                        if defense == 'baseline':
                            pred = np.argmax(baseline_model.predict(X_adv, verbose=0), axis=1)
                            detection_rate = 0.0
                        elif defense.startswith('adversarial_training'):
                            model = self.trained_models[defense]
                            pred = np.argmax(model.predict(X_adv, verbose=0), axis=1)
                            detection_rate = 0.0
                        elif defense == 'ood_denoising':
                            X_defended, detection_rate = self.apply_ood_defense(X_adv, attack_name)
                            pred = np.argmax(baseline_model.predict(X_defended, verbose=0), axis=1)
                        elif defense == 'combined_defense':
                            combined_preds, detection_rate = self.apply_combined_defense(X_adv, attack_name)
                            pred = np.argmax(combined_preds, axis=1)

                        # Calculate all metrics
                        accuracy = accuracy_score(y_true, pred)
                        precision = precision_score(y_true, pred, average='weighted', zero_division=0)
                        recall = recall_score(y_true, pred, average='weighted', zero_division=0)
                        f1 = f1_score(y_true, pred, average='weighted', zero_division=0)

                        results.append({
                            'Defense': defense,
                            'Attack': attack_name,
                            'Epsilon': epsilon,
                            'Accuracy': accuracy,
                            'Precision': precision,
                            'Recall': recall,
                            'F1_Score': f1,
                            'Detection_Rate': detection_rate
                        })

                        # Store confusion matrix for key scenarios
                        if attack_name == 'fgsm' and epsilon == 0.3 and defense in ['baseline', 'combined_defense']:
                            cm = confusion_matrix(y_true, pred)
                            confusion_matrices[f"{defense}_{attack_name}_eps{epsilon}"] = cm

                    except Exception as e:
                        logger.error(f"Error evaluating {defense} vs {attack_name}: {str(e)}")
                        results.append({
                            'Defense': defense,
                            'Attack': attack_name,
                            'Epsilon': epsilon,
                            'Accuracy': 0.0,
                            'Precision': 0.0,
                            'Recall': 0.0,
                            'F1_Score': 0.0,
                            'Detection_Rate': 0.0
                        })

        # Generate t-SNE plots
        self.generate_tsne_plots(X_test, y_test)

        # Plot confusion matrices
        self.plot_confusion_matrices(confusion_matrices)

        return pd.DataFrame(results)

    def generate_tsne_plots(self, X_test, y_test):
        """Generate t-SNE plots of latent space to show attack/defense effects"""
        logger.info("Generating t-SNE plots...")

        # Get encoder from autoencoder
        encoder = self.trained_models['autoencoder_encoder']

        # Generate adversarial examples
        baseline = self.trained_models['baseline']
        X_adv = self.generate_fgsm(baseline, X_test, 0.3)

        # Apply defense
        X_defended, _ = self.apply_ood_defense(X_adv, 'fgsm')

        # Get latent representations
        z_clean = encoder.predict(X_test, verbose=0)[2]  # z is the third output
        z_adv = encoder.predict(X_adv, verbose=0)[2]
        z_defended = encoder.predict(X_defended, verbose=0)[2]

        # Combine all data
        all_z = np.vstack([z_clean, z_adv, z_defended])
        all_labels = np.hstack([
            np.zeros(len(X_test)),  # 0 for clean
            np.ones(len(X_adv)),    # 1 for adversarial
            np.full(len(X_defended), 2)  # 2 for defended
        ])

        # Get true labels for coloring
        true_labels = np.hstack([
            np.argmax(y_test, axis=1),
            np.argmax(y_test, axis=1),
            np.argmax(y_test, axis=1)
        ])

        # Apply t-SNE
        tsne = TSNE(n_components=2, random_state=42, perplexity=30)
        z_tsne = tsne.fit_transform(all_z)

        # Create plot
        plt.figure(figsize=(15, 10))

        # Plot by data type
        plt.subplot(1, 2, 1)
        colors = ['blue', 'red', 'green']
        labels = ['Clean', 'Adversarial', 'Defended']
        for i, (color, label) in enumerate(zip(colors, labels)):
            mask = all_labels == i
            plt.scatter(z_tsne[mask, 0], z_tsne[mask, 1], c=color, label=label, alpha=0.6)
        plt.title('t-SNE by Data Type')
        plt.legend()

        # Plot by true class
        plt.subplot(1, 2, 2)
        class_colors = ['purple', 'orange']
        class_labels = ['Normal', 'Attack']
        for i, (color, label) in enumerate(zip(class_colors, class_labels)):
            mask = true_labels == i
            plt.scatter(z_tsne[mask, 0], z_tsne[mask, 1], c=color, label=label, alpha=0.6)
        plt.title('t-SNE by True Class')
        plt.legend()

        plt.tight_layout()
        plt.savefig('tsne_latent_space.png', dpi=300)
        #files.download('tsne_latent_space.png')
        plt.show()

    def plot_confusion_matrices(self, confusion_matrices):
        """Plot confusion matrices for error analysis"""
        logger.info("Generating confusion matrices...")

        fig, axes = plt.subplots(1, len(confusion_matrices), figsize=(15, 5))
        if len(confusion_matrices) == 1:
            axes = [axes]

        for i, (key, cm) in enumerate(confusion_matrices.items()):
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
            axes[i].set_title(key.replace('_', ' ').title())
            axes[i].set_xlabel('Predicted')
            axes[i].set_ylabel('True')

        plt.tight_layout()
        plt.savefig('confusion_matrices.png', dpi=300)
        #files.download('confusion_matrices.png')
        plt.show()

    def create_attack_vs_epsilon_plots(self, results_df):
        """Enhanced visualization of attack strength vs defense performance"""
        plt.style.use('seaborn-v0_8-darkgrid')
        fig, axes = plt.subplots(2, 3, figsize=(20, 12))
        fig.suptitle('Attack Strength (ε) vs Defense Performance', fontsize=16, y=1.02)

        # Defense style configuration
        defense_styles = {
            'baseline': {'color': 'red', 'marker': 'o', 'linestyle': '--'},
            'adversarial_training_1': {'color': 'orange', 'marker': 's', 'linestyle': '-'},
            'adversarial_training_2': {'color': 'gold', 'marker': 'D', 'linestyle': '-'},
            'adversarial_training_3': {'color': 'green', 'marker': '^', 'linestyle': '-'},
            'ood_denoising': {'color': 'blue', 'marker': 'p', 'linestyle': ':'},
            'combined_defense': {'color': 'purple', 'marker': '*', 'linestyle': '-'}
        }

        attack_types = sorted(results_df['Attack'].unique())
        for i, attack in enumerate(attack_types):
            ax = axes[i//3, i%3] if len(attack_types) > 3 else axes[i]
            attack_data = results_df[results_df['Attack'] == attack]
            for defense in defense_styles.keys():
                defense_data = attack_data[attack_data['Defense'] == defense]
                if not defense_data.empty:
                    ax.plot(defense_data['Epsilon'], defense_data['F1_Score'],
                           label=defense,
                           **defense_styles[defense])
            ax.set_xlabel('Attack Strength (ε)', fontsize=10)
            ax.set_ylabel('F1 Score', fontsize=10)
            ax.set_title(f'{attack.upper()} Attack', fontsize=12)
            ax.grid(True, alpha=0.3)
            ax.set_ylim(0, 1.05)

            # Add trend line for baseline
            baseline_data = attack_data[attack_data['Defense'] == 'baseline']
            if not baseline_data.empty:
                z = np.polyfit(baseline_data['Epsilon'], baseline_data['F1_Score'], 1)
                p = np.poly1d(z)
                ax.plot(baseline_data['Epsilon'], p(baseline_data['Epsilon']),
                        color='red', alpha=0.3, linestyle='--')

        # Create unified legend
        legend_elements = [Line2D([0], [0],
                           color=style['color'],
                           marker=style['marker'],
                           linestyle=style['linestyle'],
                           label=defense.replace('_', ' ').title())
                          for defense, style in defense_styles.items()]
        fig.legend(handles=legend_elements,
                  loc='lower center',
                  ncol=len(defense_styles),
                  bbox_to_anchor=(0.5, -0.05),
                  fontsize=10)

        plt.tight_layout()
        plt.savefig('attack_vs_epsilon_performance.png', dpi=300, bbox_inches='tight')
        #files.download('attack_vs_epsilon_performance.png')
        plt.show()

    def create_defense_heatmap(self, results_df):
        """Heatmap showing defense effectiveness across attacks"""
        plt.figure(figsize=(12, 8))

        # Calculate improvement over baseline
        heatmap_data = []
        for attack in results_df['Attack'].unique():
            attack_data = results_df[results_df['Attack'] == attack]
            baseline_mean = attack_data[attack_data['Defense'] == 'baseline']['F1_Score'].mean()
            for defense in attack_data['Defense'].unique():
                if defense != 'baseline':
                    defense_mean = attack_data[attack_data['Defense'] == defense]['F1_Score'].mean()
                    improvement = defense_mean - baseline_mean
                    heatmap_data.append({
                        'Attack': attack,
                        'Defense': defense.replace('_', ' ').title(),
                        'Improvement': improvement
                    })

        heatmap_df = pd.DataFrame(heatmap_data)
        pivot_table = heatmap_df.pivot(index="Defense", columns="Attack", values="Improvement")

        # Create heatmap using Matplotlib
        plt.imshow(pivot_table, cmap='coolwarm', interpolation='nearest')
        plt.colorbar(label='F1 Score Improvement Over Baseline')
        plt.xticks(np.arange(len(pivot_table.columns)), pivot_table.columns, rotation=45)
        plt.yticks(np.arange(len(pivot_table.index)), pivot_table.index)
        plt.title('Defense Effectiveness Across Attack Types', pad=20)

        # Add annotations
        for i in range(len(pivot_table.index)):
            for j in range(len(pivot_table.columns)):
                plt.text(j, i, f"{pivot_table.iloc[i, j]:.2f}",
                        ha='center', va='center', color='black')

        plt.tight_layout()
        plt.savefig('defense_effectiveness_heatmap.png', dpi=300, bbox_inches='tight')
        #files.download('defense_effectiveness_heatmap.png')
        plt.show()

    def plot_detection_rates(self, results_df):
        """Visualize OOD detection rates across attacks"""
        plt.figure(figsize=(12, 6))

        # Filter for defenses with detection capability
        detection_data = results_df[
            (results_df['Defense'].isin(['ood_denoising', 'combined_defense'])) &
            (results_df['Detection_Rate'] > 0)
        ]

        attacks = detection_data['Attack'].unique()
        defenses = ['ood_denoising', 'combined_defense']
        colors = ['blue', 'purple']
        bar_width = 0.35

        for i, defense in enumerate(defenses):
            defense_data = detection_data[detection_data['Defense'] == defense]
            x = np.arange(len(attacks)) + i * bar_width
            heights = [defense_data[defense_data['Attack'] == attack]['Detection_Rate'].mean()
                      for attack in attacks]
            plt.bar(x, heights, bar_width, label=defense, color=colors[i])

            # Add value labels
            for j, height in enumerate(heights):
                plt.text(x[j], height + 0.02, f"{height:.2f}", ha='center', va='bottom')

        plt.xlabel('Attack Type', fontsize=12)
        plt.ylabel('Detection Rate', fontsize=12)
        plt.title('Adversarial Sample Detection Rates', fontsize=14)
        plt.xticks(np.arange(len(attacks)) + bar_width / 2, attacks)
        plt.ylim(0, 1)
        plt.grid(True, axis='y', alpha=0.3)
        plt.legend(title='Defense Method')
        plt.tight_layout()
        plt.savefig('detection_rate_analysis.png', dpi=300)
        #files.download('detection_rate_analysis.png')
        plt.show()

    def create_comprehensive_visualizations(self, results_df):
        """Complete visualization pipeline"""
        logger.info("Creating enhanced visualizations...")
        # 1. Attack vs Epsilon plots
        self.create_attack_vs_epsilon_plots(results_df)
        # 2. Defense effectiveness heatmap
        self.create_defense_heatmap(results_df)
        # 3. Detection rate analysis
        self.plot_detection_rates(results_df)

    def print_comprehensive_analysis(self, results_df):
        """Complete results analysis with additional metrics"""
        print("\n" + "="*120)
        print("COMPREHENSIVE ADVERSARIAL DEFENSE EVALUATION - FINAL RESULTS")
        print("="*120)

        # Overall statistics
        print(f"\nTotal evaluations performed: {len(results_df)}")
        print(f"Attacks tested: {', '.join(results_df['Attack'].unique())}")
        print(f"Defense mechanisms: {len(results_df['Defense'].unique())}")

        # Create pivot table for better readability
        pivot_f1 = results_df.pivot_table(
            index=['Attack', 'Epsilon'],
            columns='Defense',
            values='F1_Score',
            aggfunc='mean'
        )

        print("\n" + "="*100)
        print("F1 SCORE RESULTS TABLE")
        print("="*100)
        print(pivot_f1.round(3).to_string())

        # Find meaningful attack scenarios (where baseline F1 < 0.8)
        meaningful_results = results_df[
            (results_df['Defense'] == 'baseline') &
            (results_df['F1_Score'] < 0.8)
        ]

        if not meaningful_results.empty:
            print("\n" + "="*80)
            print("DEFENSE EFFECTIVENESS ANALYSIS (Meaningful Attack Scenarios)")
            print("="*80)
            meaningful_attacks = meaningful_results[['Attack', 'Epsilon']].to_records(index=False)
            for attack, epsilon in meaningful_attacks:
                scenario_data = results_df[
                    (results_df['Attack'] == attack) &
                    (results_df['Epsilon'] == epsilon)
                ]
                print(f"\n{attack.upper()} Attack at ε={epsilon}:")
                scenario_summary = scenario_data.set_index('Defense')['F1_Score'].round(3)
                print(scenario_summary.to_string())

                # Find best defense for this scenario
                best_defense = scenario_data.loc[scenario_data['F1_Score'].idxmax()]
                baseline_f1 = scenario_data[scenario_data['Defense'] == 'baseline']['F1_Score'].iloc[0]
                improvement = best_defense['F1_Score'] - baseline_f1
                print(f"Best Defense: {best_defense['Defense']} (F1: {best_defense['F1_Score']:.3f}, "
                      f"Improvement: {improvement:+.3f})")
        else:
            print("\n⚠️  No meaningful attack scenarios found (all baseline F1 scores > 0.8)")
            print("Consider using higher epsilon values or more aggressive attacks.")

        # Defense ranking
        print("\n" + "="*80)
        print("OVERALL DEFENSE RANKING")
        print("="*80)
        defense_performance = results_df.groupby('Defense')['F1_Score'].agg(['mean', 'std']).round(3)
        defense_performance = defense_performance.sort_values('mean', ascending=False)
        print("Defense Performance (sorted by mean F1 score):")
        print(defense_performance.to_string())

        # Improvement over baseline
        baseline_mean = results_df[results_df['Defense'] == 'baseline']['F1_Score'].mean()
        print(f"\nImprovement over baseline (mean F1 score: {baseline_mean:.3f}):")
        for defense in ['adversarial_training_3', 'ood_denoising', 'combined_defense']:
            if defense in results_df['Defense'].values:
                defense_mean = results_df[results_df['Defense'] == defense]['F1_Score'].mean()
                improvement = defense_mean - baseline_mean
                print(f"  {defense.replace('_', ' ').title()}: {improvement:+.3f}")

# 3. Main Pipeline
def main_improved_evaluation():
    """Complete evaluation pipeline."""
    # === Data Loading and Preparation ===
    X_train, X_val, X_test, y_train, y_val, y_test, scaler, label_encoder = load_and_prepare_data(filepath="Bot-IoT-Training.csv")

    # === Defense/Attack Training and Evaluation ===
    framework = ImprovedAdversarialDefenseFramework()
    framework.train_all_models(X_train, y_train, X_val, y_val)

    # Comprehensive evaluation
    results_df = framework.comprehensive_evaluation(X_test, y_test)

    # Visualizations
    framework.create_comprehensive_visualizations(results_df)

    # Console analysis/report
    framework.print_comprehensive_analysis(results_df)

    results_df.to_csv('final_comprehensive_defense_results.csv', index=False)
    logger.info("Results saved to 'final_comprehensive_defense_results.csv'")

    return framework, results_df

if __name__ == "__main__":
    framework, results = main_improved_evaluation()